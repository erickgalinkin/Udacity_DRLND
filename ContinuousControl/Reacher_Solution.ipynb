{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control\n",
    "## Introduction\n",
    "\n",
    "The goal of this project is to use a double-jointed arm to move to target locations in a continuous world and maintain its position at the target location for as many time steps as possible. The reward function provides a +0.1 reward for as many time steps as the agent's hand is in the target location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Number of actions: 4\n",
      "States look like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n",
      "States have length: 33\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D4PG Agent\n",
    "For this agent, we elect to implement the Distributed Distributional Deep Deterministic Policy Gradients (D4PG) algorithm as developed by [Barth-Maron, Hoffman, et al.](https://arxiv.org/pdf/1804.08617.pdf). The D4PG model uses an Actor-Critic method based on the original Deep Deterministic Policy Gradients (DDPG) [paper](https://arxiv.org/pdf/1509.02971.pdf), but by using a distributional critic, the learning is more stable. In order to speed-up learning, since the algorithm is off-policy, we distribute the experience gathering task across 20 actors.\n",
    "\n",
    "These actors are neural networks with 2 hidden layers which map states to action values and whose outputs are activated with a tanh activation to keep it in the range of \\[-1, 1\\]. The critic takes the form of a neural network which allows for the input of a tuple (state, action) and outputs the Q value of the pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of neurons in the first hidden layer\n",
    "            fc2_units (int): Number of neurons in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values\"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of neurons in the first hidden layer\n",
    "            fc2_units (int): Number of neurons in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a network that maps (state, action) -> Q values\"\"\"\n",
    "        xs = F.leaky_relu(self.fc1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \"\"\"Buffer to replay experience tuples\"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize ReplayBuffer class\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): Dimension of each action\n",
    "            buffer_size (int): Length of replay buffer\n",
    "            batch_size (int): Size of each training mini-batch.\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Append an experience to memory\"\"\"\n",
    "        exp = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(exp)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the memory\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck noise process to be added to the actions.\"\"\"\n",
    "\n",
    "    def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class D4PG_Agent():\n",
    "    def __init__(self, state_size, action_size, buffer_size=int(1e5), batch_size=1024, update_every = 4, \n",
    "                 num_actors=20, alpha=.0001, beta=.0005, gamma=.95, tau=1e-3, epsilon=.99):\n",
    "        \"\"\" Initialize agent attributes\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            buffer_size (int): Size of replay buffer\n",
    "            batch_size (int): Batch size for replay buffer\n",
    "            num_actors(int): Number of simultaneous actors\n",
    "            alpha (float): Learning rate for actor\n",
    "            beta (float): Learning rate for critic\n",
    "            tau (float): Soft update multiplier\n",
    "            epsilon (float): Parameter for controlling exploration vs exploitation\n",
    "        \"\"\"\n",
    "        self.num_actors = num_actors\n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.local_actor = Actor(state_size, action_size).to(device)\n",
    "        self.target_actor = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.local_actor.parameters(), lr=alpha)\n",
    "        self.local_critic = Critic(state_size, action_size).to(device)\n",
    "        self.target_critic = Critic(state_size, action_size).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.local_critic.parameters(), lr=beta)\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size)\n",
    "        self.noise = OUNoise(action_size)\n",
    "        self.timesteps = 0\n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay buffer, use random sample from buffer to learn\n",
    "        Params\n",
    "        ======\n",
    "            state (ndarray): State of the environment\n",
    "            action (ndarray): Action chosen by the agent\n",
    "            reward (ndarray): Reward given by the environment\n",
    "            next_state (ndarray): Next state of the environment\n",
    "            done (ndarray): Flag to indicate if the episode is finished after this action\n",
    "        \"\"\"\n",
    "        self.timesteps += 1\n",
    "        self.memory.add_experience(state, action, reward, next_state, done)\n",
    "        update_target_net = False\n",
    "        \n",
    "        if len(self.memory) > self.batch_size and self.timesteps % self.update_every == 0:\n",
    "            batch = self.memory.sample()\n",
    "            if self.timesteps % (5 * self.update_every) == 0:\n",
    "                update_target_net = True\n",
    "            self.learn(batch, self.gamma, update_target_net)\n",
    "                \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Given a state, choose an action to take\n",
    "        Params\n",
    "        ======\n",
    "            state (ndarray): State of the environment\n",
    "            score (float): Current score\n",
    "            max_score (float): Maximum possible score\n",
    "            add_noise (bool): Flag indicating whether or not to add ou noise to the environment\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        \n",
    "        self.local_actor.eval() # Set local network to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            action = self.local_actor(state).cpu().data.numpy()\n",
    "        self.local_actor.train()\n",
    "            \n",
    "        if add_noise:\n",
    "            action += [self.epsilon * self.noise.sample() for _ in range(self.num_actors)]\n",
    "                \n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            local_model (nn.Module): model to copy weights from\n",
    "            target_model (nn.Module): model to copy weights to\n",
    "        \"\"\"\n",
    "        \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1-self.tau)*target_param.data)\n",
    "    \n",
    "    def learn(self, batch, gamma, update_target_net=True):\n",
    "        \"\"\" Given a batch of experiences, update the local network and soft update on target networks.\n",
    "        Q = r + gamma * critic_target(next_state, actor_target(next_state))\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            batch (tuple of tensors): Experiences - (states, actions, rewards, next_states, dones)\n",
    "            gamma (float): Discount factor for rewards\n",
    "            update_target_net (bool): Whether or not to update the target net (as opposed to local)\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        # Compute critic loss\n",
    "        Q_expected = self.local_critic(states, actions)\n",
    "        Q_target_next = self.target_critic(next_states, self.target_actor(next_states))\n",
    "        Q_target = rewards + (gamma * Q_target_next * (1-dones))\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_target)\n",
    "        \n",
    "        # Minimize loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.local_critic.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Compute actor loss\n",
    "        Q_local = self.local_critic(states, self.local_actor(states))\n",
    "        actor_loss = - Q_local.mean()\n",
    "        \n",
    "        # Actor gradient ascent\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Reduce exploration\n",
    "        self.epsilon *= self.epsilon\n",
    "        \n",
    "        # Soft update\n",
    "        if update_target_net:\n",
    "            self.soft_update(self.local_critic, self.target_critic)\n",
    "            self.soft_update(self.local_actor, self.target_actor)\n",
    "            \n",
    "    def reset(self):\n",
    "        self.epsilon = .99\n",
    "        self.noise.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = D4PG_Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 complete! Average score: 0.48\tEpisode score: 0.48\n",
      "Episode 10 complete! Average score: 0.75\tEpisode score: 0.83\n",
      "Episode 20 complete! Average score: 1.29\tEpisode score: 2.91\n",
      "Episode 30 complete! Average score: 4.07\tEpisode score: 14.05\n",
      "Episode 40 complete! Average score: 9.08\tEpisode score: 30.04\n",
      "Episode 50 complete! Average score: 13.39\tEpisode score: 30.64\n",
      "Episode 60 complete! Average score: 16.25\tEpisode score: 29.63\n",
      "Episode 70 complete! Average score: 18.23\tEpisode score: 30.32\n",
      "Episode 80 complete! Average score: 19.75\tEpisode score: 30.03\n",
      "Episode 90 complete! Average score: 20.94\tEpisode score: 29.69\n",
      "Episode 100 complete! Average score: 21.84\tEpisode score: 30.98\n",
      "Episode 110 complete! Average score: 24.79\tEpisode score: 30.31\n",
      "Episode 120 complete! Average score: 27.59\tEpisode score: 29.09\n",
      "Episode 130 complete! Average score: 29.62\tEpisode score: 30.23\n",
      "Target reward achieved in 134 episodes! Average score: 30.070164\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 500\n",
    "current_score = 0\n",
    "scores = list()\n",
    "rolling_average = list()\n",
    "score_deque = deque(maxlen=100)\n",
    "\n",
    "for ep in range(1, n_episodes+1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    agent.reset()\n",
    "    episode_scores = np.zeros(agent.num_actors)\n",
    "    for _ in range(800):\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        \n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        \n",
    "        episode_scores += rewards\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "            \n",
    "    episode_score = np.mean(episode_scores)\n",
    "    score_deque.append(episode_score)\n",
    "    scores.append(episode_score)\n",
    "    ra_current = np.mean(score_deque)\n",
    "    rolling_average.append(ra_current)\n",
    "    \n",
    "    if ep == 1 or ep % 10 == 0:\n",
    "        print(\"Episode {} complete! Average score: {:.2f}\\tEpisode score: {:.2f}\".format(ep, ra_current, episode_score))\n",
    "    \n",
    "    if ra_current >= 30 and len(score_deque) > 99:\n",
    "        print(\"Target reward achieved in {} episodes! Average score: {:.2f}\".format(ep, ra_current))\n",
    "        break\n",
    "        \n",
    "torch.save(agent.target_actor.state_dict(), 'actor_solution.pth')\n",
    "torch.save(agent.target_critic.state_dict(), 'critic_solution.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.savefig('continuous_control_scores.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

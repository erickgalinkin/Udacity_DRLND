{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Collaboration and Competition\n",
    "## Introduction\n",
    "The goal of this project is to train two competitive agents to play tennis against each other. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. So, our goal is to keep the ball volleying as long as possible. We consider the environment solved when the agents achieve an average score of +0.5 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Linux_NoVis/Tennis.x86_64\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Agent Deep Deterministic Policy Gradient Agent\n",
    "We implement the [Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/abs/1509.02971) algorithm for this agent. DDPG is an adaptation of Deep Q-learning to a continuous action space and we can adapt this to our multi-agent environment using techniques introduced by [Lowe et al.](https://arxiv.org/abs/1706.02275) to conduct multi-agent coordination. We could have used D4PG as in our previous project as well.\n",
    "\n",
    "The actors are neural networks with two hidden layers whose outputs are activated with tanh to produce an output in the range \\[-1, 1\\]. The critic accepts a (state, action) tuple and outputs the Q value learned for that pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_units=512, fc2_units=256):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of neurons in the first hidden layer\n",
    "            fc2_units (int): Number of neurons in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size*2, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values\"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_units=512, fc2_units=256):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of neurons in the first hidden layer\n",
    "            fc2_units (int): Number of neurons in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size*2, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units+(action_size*2), fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a network that maps (state, action) -> Q values\"\"\"\n",
    "        xs = F.relu(self.fc1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \"\"\"Buffer to replay experience tuples\"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize ReplayBuffer class\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): Dimension of each action\n",
    "            buffer_size (int): Length of replay buffer\n",
    "            batch_size (int): Size of each training mini-batch.\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Append an experience to memory\"\"\"\n",
    "        exp = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(exp)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the memory\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck noise process to be added to the actions.\"\"\"\n",
    "\n",
    "    def __init__(self, size, mu=0.0, theta=0.15, sigma=0.3):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG_Agent:\n",
    "    def __init__(self, state_size, action_size, buffer_size=int(1e5), batch_size=512, update_every=5, \n",
    "                 no_agents=2, alpha=.01, beta=.03, gamma=.99, tau=.0001, epsilon=.99):\n",
    "        \"\"\" Initialize agent attributes\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            buffer_size (int): Size of replay buffer\n",
    "            batch_size (int): Batch size for replay buffer\n",
    "            no_agents (int): Number of agents\n",
    "            alpha (float): Learning rate for actor\n",
    "            beta (float): Learning rate for critic\n",
    "            tau (float): Soft update multiplier\n",
    "            epsilon (float): Parameter for controlling exploration vs exploitation\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.update_every = update_every\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.no_agents = no_agents\n",
    "        \n",
    "        self.local_actor = Actor(state_size, action_size).to(device)\n",
    "        self.target_actor = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.local_actor.parameters(), lr=alpha)\n",
    "        \n",
    "        self.local_critic = Critic(state_size, action_size).to(device)\n",
    "        self.target_critic = Critic(state_size, action_size).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.local_critic.parameters(), lr=beta)\n",
    "        \n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size)\n",
    "        self.noise = OUNoise((no_agents, action_size))\n",
    "        self.timesteps = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done, agent_no):\n",
    "        \"\"\"Save experience in replay buffer, use random sample from buffer to learn\n",
    "        Params\n",
    "        ======\n",
    "            state (ndarray): State of the environment\n",
    "            action (ndarray): Action chosen by the agent\n",
    "            reward (ndarray): Reward given by the environment\n",
    "            next_state (ndarray): Next state of the environment\n",
    "            done (ndarray): Flag to indicate if the episode is finished after this action\n",
    "            agent_no (int): Which agent (1 or 2) performs the step\n",
    "        \"\"\"\n",
    "        self.timesteps += 1\n",
    "        self.memory.add_experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.memory) >= self.batch_size and self.timesteps % self.update_every == 0:\n",
    "            batch = self.memory.sample()\n",
    "            self.learn(batch, self.gamma, agent_no)\n",
    "    \n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Given a state, choose an action to take\n",
    "        Params\n",
    "        ======\n",
    "            state (ndarray): State of the environment\n",
    "            score (float): Current score\n",
    "            add_noise (bool): Flag indicating whether or not to add ou noise to the environment\n",
    "        \"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = np.zeros((self.no_agents, self.action_size))\n",
    "        \n",
    "        self.local_actor.eval() # Set local network to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for agent_no, state in enumerate(states):\n",
    "                action = self.local_actor(state).cpu().data.numpy()\n",
    "                actions[agent_no, :] = action\n",
    "        self.local_actor.train()\n",
    "            \n",
    "        if add_noise:\n",
    "            actions += self.epsilon * self.noise.sample()\n",
    "                \n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            local_model (nn.Module): model to copy weights from\n",
    "            target_model (nn.Module): model to copy weights to\n",
    "        \"\"\"\n",
    "        \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1-self.tau)*target_param.data)\n",
    "    \n",
    "    def learn(self, batch, gamma, agent_no):\n",
    "        \"\"\" Given a batch of experiences, update the local network and soft update on target networks.\n",
    "        Q = r + gamma * critic_target(next_state, actor_target(next_state))\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            batch (tuple of tensors): Experiences - (states, actions, rewards, next_states, dones)\n",
    "            gamma (float): Discount factor for rewards\n",
    "            agent_no (int): Which agent to update\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        next_actions = self.target_actor(next_states)\n",
    "        \n",
    "        if agent_no == 0:\n",
    "            next_actions = torch.cat((next_actions, actions[:,2:]), dim=1)\n",
    "        else:\n",
    "            next_actions = torch.cat((actions[:,:2], next_actions), dim=1)\n",
    "\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.local_critic(states, actions)\n",
    "        Q_target_next = self.target_critic(next_states, next_actions)\n",
    "        Q_target = rewards + (gamma * Q_target_next * (1 - dones))\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_target)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.local_critic.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Compute actor loss\n",
    "        actions_pred = self.local_actor(states)\n",
    "        if agent_no == 0:\n",
    "            actions_pred = torch.cat((actions_pred, actions[:,2:]), dim=1)\n",
    "        else:\n",
    "            actions_pred = torch.cat((actions[:,:2], actions_pred), dim=1)\n",
    "        \n",
    "        actor_loss = -self.local_critic(states, actions_pred).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Reduce exploration\n",
    "        self.epsilon *= self.epsilon\n",
    "        \n",
    "        # Soft update\n",
    "        self.soft_update(self.local_critic, self.target_critic)\n",
    "        self.soft_update(self.local_actor, self.target_actor)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.epsilon = .99\n",
    "        self.noise.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent0 = MADDPG_Agent(state_size, action_size)\n",
    "agent1 = MADDPG_Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 complete! Average score: 0.00\tEpisode score: 0.00\n",
      "Episode 50 complete! Average score: 0.00\tEpisode score: 0.00\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 500\n",
    "current_score = 0\n",
    "scores = list()\n",
    "rolling_average = list()\n",
    "score_deque = deque(maxlen=100)\n",
    "\n",
    "for ep in range(1, n_episodes+1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = np.reshape(env_info.vector_observations, (1,48))\n",
    "    agent0.reset()\n",
    "    agent1.reset()\n",
    "    episode_scores = np.zeros(len(env_info.agents))\n",
    "    while True:\n",
    "        action_0 = agent0.act(states)\n",
    "        action_1 = agent1.act(states)\n",
    "        actions = np.concatenate((action_0, action_1), axis=0).flatten()\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        \n",
    "        next_states = np.reshape(env_info.vector_observations, (1, 48))\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        episode_scores += np.max(rewards)\n",
    "        \n",
    "        agent0.step(states, actions, rewards[0], next_states, dones[0], 0)\n",
    "        agent1.step(states, actions, rewards[1], next_states, dones[1], 1)\n",
    "\n",
    "        states = next_states\n",
    "        if np.max(rewards) > 0:\n",
    "            print(rewards)\n",
    "        \n",
    "        if np.any(dones):\n",
    "            break\n",
    "            \n",
    "    episode_score = np.max(episode_scores)\n",
    "    score_deque.append(episode_score)\n",
    "    scores.append(episode_score)\n",
    "    ra_current = np.mean(score_deque)\n",
    "    rolling_average.append(ra_current)\n",
    "    if ep == 1 or ep % 50 == 0:\n",
    "        print(\"Episode {} complete! Average score: {:.2f}\\tEpisode score: {:.2f}\".format(ep, ra_current, episode_score))\n",
    "    \n",
    "    if ra_current >= 0.5 and len(score_deque) > 99:\n",
    "        print(\"Target reward achieved in {} episodes! Average score: {:.2f}\".format(ep, ra_current))\n",
    "        break\n",
    "        \n",
    "torch.save(agent0.target_actor.state_dict(), 'agent0_actor_solution.pth')\n",
    "torch.save(agent0.target_critic.state_dict(), 'agent0_critic_solution.pth')\n",
    "torch.save(agent1.target_actor.state_dict(), 'agent1_actor_solution.pth')\n",
    "torch.save(agent1.target_critic.state_dict(), 'agent1_critic_solution.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
